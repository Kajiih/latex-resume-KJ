% % Experience section
\companyNameAndLocationHeading{
    SNU Vision \& Learning Lab
}{
    \IfLanguageName{french}{Séoul, Corée du Sud}{Seoul, South Korea}
}
\titleAndDateHeading{
    \IfLanguageName{french}{Étudiant-chercheur}{Graduate Student Researcher}
}{
    \september{} 2022 \textbf{--} \september{} 2024
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        \ifthenelse{\equal{\resumeFocus}{eng}}{
            Développement et optimisation d'un modèle d'extraction de compétences en JAX/Flax via apprentissage par renforcement avec rétroaction humaine (RLHF), réduisant les coûts d'entraînement de 60~\%.
        }{
            Amélioration de la qualité et de l'alignement de la découverte non supervisée de compétences via l'apprentissage par renforcement avec rétroaction humaine (RLHF) pour des intelligences artificielles (IA) incarnées.
            Implémentation et optimisation d'un modèle probabiliste d'extraction de compétences en JAX/Flax, réduisant les coûts d'entraînement de 60~\%.
        }%
        Formation et accompagnement des chercheurs dans l'adoption de JAX, atteignant 25~\% d'adoption en un an.
    }{
        \ifthenelse{\equal{\resumeFocus}{eng}}{
            Developed and optimized a probabilistic graphical model for unsupervised skill extraction in JAX/Flax using reinforcement learning with human feedback (RLHF), reducing training costs by 60\%.
        }{
            % Spearheaded RLHF experiments to improve unsupervised skill discovery in embodied environments (iTHOR, Crafter).
            % Engineered a graphical skill extraction model in JAX/Flax, boosting training speed by 2–4x.
            Conducted reinforcement learning with human feedback (RLHF) experiments to enhance quality and alignment of unsupervised skill discovery in two embodied environments (iTHOR, Crafter).
            Fully implemented a graphical skill extraction model in JAX/Flax, achieving a 2--4x increase in training speed.
        }%
        % Led lab-wide adoption of JAX, mentoring other researchers and driving \midTilde 25\% adoption within a year.  % ChkTeX 1
        Mentored researchers on adopting JAX, achieving \midTilde25\% lab-wide adoption within one year.

    }
}
\bulletItem{
    % ce dont je veux parler (pas forcément dans l'ordre):
    % developpement de l'environnement avec un framework pour creer des taches modulaire, et Conception de nouveaux algorithmes permettant de rendre calculables la fonction d'evaluation des agents, réduisant la surcharge à 1,7 % du temps d'exécution brut d'AI2THOR. deploimend de docker pour permettre l'entrainement des agents sur un cluster sans interface graphique
    \IfLanguageName{french}{
        Conception et développement d'un environnement d'apprentissage par renforcement incarné basé sur AI2THOR, incluant un framework pour la création de tâches composables.
        Conception d'algorithmes rendant calculable la fonction d'évaluation des agents, réduisant la surcharge à 1,7~\% du temps d'exécution brut d'AI2THOR\@.
        % Optimisation des algorithmes d'évaluation des agents, réduisant la surcharge à 1,7\% du temps d'exécution brut d'AI2THOR\@.
        Déploiement de conteneurs Docker pour permettre l'entraînement des agents sur un cluster sans interface graphique.
    }{
        % Led the design and development of an embodied reinforcement learning (RL) environment based on AI2THOR\@.
        % Refined task evaluation algorithms, cutting overhead to 1.7\% of the raw simulation runtime.
        Led the design and development of an embodied reinforcement learning (RL) environment based on AI2THOR, including a framework for creating realistic hierachical tasks.
        % Refined task evaluation algorithms, reducing overhead to 1.7\% of the raw simulation runtime in AI2THOR\@.
        Designed algorithms making the agent evaluation function tractable, reducing overhead to 1.7\% of raw simulation runtime in AI2THOR\@.
        Deployed Docker containers to run Unity simulations on a headless cluster, enabling efficient training of RL agents.
        % Designed 14 complex RL tasks, increasing task completion rates by 80\% compared to heuristic-based rewards.
    }
}
\outerBulletListEnd{}


\companyNameAndLocationHeading{
    Apex Solutions
}{
    Figeac, France
}
\titleAndDateHeading{
    \IfLanguageName{french}{%
        \ifthenelse{\equal{\resumeFocus}{eng}}{Ingénieur Stagiaire}{Stage de recherche}
    }{%
        \ifthenelse{\equal{\resumeFocus}{eng}}{AI Engineer Internship}{Research Internship}
    }
}{
    \june{} 2022 \textbf{--} \august{} 2022
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        Développement d'un prototype d'environnement multi-agent pour la simulation de scénarios d'intrusion dans des infrastructures critiques (Python, Gym/Gymnasium), toujours activement utilisé pour la recherche interne.
    }{
        Prototyped a multi-agent simulation environment in Python (Gym/Gymnasium) to model intrusion scenarios in critical infrastructure.
        Deployed a foundational framework still actively used for internal security research.
    }
}

\bulletItem{
    \IfLanguageName{french}{
        Conception et implémentation de benchmarks en apprentissage par renforcement (RL) pour des simulations multi-agents de capture-the-flag avec partage d'information limité (PyTorch, Stable-Baselines3).
        Adaptation et optimisation d'algorithmes de RL pour les agents d'attaque et de défense dans un contexte multi-agent.
    }{
        Developed RL benchmarks for multi-agent capture-the-flag simulations with realistic limited information-sharing constraints (PyTorch, Stable-Baselines3).
        Adapted and optimized RL algorithms for red team (offense) and blue team (defense) in multi-agent scenarios, enhancing adversarial training in security-focused simulations.
    }
}
\outerBulletListEnd{}


% \iftoggle{is_long_version}
% {
%
% }{}
