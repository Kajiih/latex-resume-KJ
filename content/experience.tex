% % Experience section
\companyNameAndLocationHeading{
    SNU Vision \& Learning Lab
}{
    \IfLanguageName{french}{Séoul, Corée du Sud}{Seoul, South Korea}
}
\titleAndDateHeading{
    \IfLanguageName{french}{Étudiant-chercheur}{Graduate Student Researcher}
}{
    \september{} 2022 \textbf{--} \september{} 2024
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        Recherche en apprentissage par renforcement avec rétroaction humaine (RLHF) pour améliorer la qualité et l'alignement de la découverte non supervisée de compétences dans deux environnements incarnés (iTHOR, Crafter).
        Implémentation complète d'un modèle probabiliste d'extraction de compétences en JAX/Flax, réduisant les coûts d'entraînement par un facteur 3.
        Facilitée l'adoption du framework JAX au sein du laboratoire en formant d'autres chercheurs, atteignant environ 25\% d'adoption en un an.
    }{
        Conducted reinforcement learning with human feedback (RLHF) experiments to enhance quality and alignment of unsupervised skill discovery in two embodied environments (iTHOR, Crafter).
        Fully implemented a graphical skill extraction model in JAX/Flax, achieving a 2--4x increase in training speed.
        Facilitated lab-wide adoption of the JAX framework by introducing and supporting other researchers in its application, achieving \~25\% adoption within a year.
    }

}
\bulletItem{
    \IfLanguageName{french}{
        Conception et développement d'un environnement d'apprentissage par renforcement incarné basé sur AI2THOR\@.
        Déploiement de conteneurs Docker pour exécuter des simulations Unity sur un cluster sans interface graphique, optimisant l'entraînement des agents.
        Optimisation des algorithmes d'évaluation, réduisant la surcharge à 1,7\% du temps d'exécution brut d'AI2THOR\@.
        % Conception de 14 tâches RL complexes, montrant une amélioration relative de 80\% du taux de succès grâce à un signal de récompense basé sur des prédicats.
    }{
        Led the design and development of an embodied reinforcement learning (RL) environment based on AI2THOR
        Deployed Docker containers to run Unity-based simulations on a headless cluster, enabling efficient training of RL agents.
        Optimized task evaluation algorithms, reducing overhead to 1.7\% of the raw AI2THOR simulation runtime.
        Designed 14 complex RL tasks, demonstrating 80\% relative improvement in task completion with a predicate-based reward signal.
    }
}
\outerBulletListEnd{}


\companyNameAndLocationHeading{
    Apex Solutions
}{
    Figeac, France
}
\titleAndDateHeading{
    \IfLanguageName{french}{Stage de recherche}{Research Internship}
}{
    \june{} 2022 \textbf{--} \august{} 2022
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        Initié le développement d'un environnement multi-agent pour la simulation de scénarios d'intrusion dans des infrastructures critiques (Python, Gym/Gymnasium).
    }{
        Initiated the development of a multi-agent environment to simulate intrusion scenarios in critical infrastructure (Python, Gym/Gymnasium).
        Delivered a base environment still actively used in subsequent research projects.
    }
}

\bulletItem{
    \IfLanguageName{french}{
        Conception et implémentation de benchmarks en apprentissage par renforcement (RL) pour des simulations multi-agents de capture-the-flag avec partage d'information limité (PyTorch, Stable-Baselines3).
        Adaptation des algorithmes de RL pour les agents \textit{red team} (attaque) et \textit{blue team} (défense) dans un contexte multi-agent.
    }{
        Designed and implemented reinforcement learning (RL) benchmarks for multi-agent capture-the-flag simulations with limited information-sharing constraints (PyTorch, Stable-Baselines3).
        Adapted RL algorithms for red team (penetration) and blue team (defense) agents to optimize performance in multi-agent scenarios.
    }
}
\outerBulletListEnd{}


% \iftoggle{is_long_version}
% {
%
% }{}
