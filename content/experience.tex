% % Experience section
\companyNameAndLocationHeading{
    SNU Vision \& Learning Lab
}{
    \IfLanguageName{french}{Séoul, Corée du Sud}{Seoul, South Korea}
}
\titleAndDateHeading{
    \IfLanguageName{french}{Étudiant-chercheur}{Graduate Student Researcher}
}{
    \september{} 2022 \textbf{--} \september{} 2024
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        \ifthenelse{\equal{\resumeFocus}{dev}}{
            Développement et optimisation d'un modèle d'extraction de compétences en JAX/Flax via apprentissage par renforcement avec rétroaction humaine (RLHF), réduisant les coûts d'entraînement de 60~\%.
        }{
            Amélioration de la qualité et de l'alignement de la découverte non supervisée de compétences via l'apprentissage par renforcement avec rétroaction humaine (RLHF) pour des intelligences artificielles (IA) incarnées.
            Implémentation et optimisation d'un modèle probabiliste d'extraction de compétences en JAX/Flax, réduisant les coûts d'entraînement de 60\%.
        }%
        Formation et accompagnement des chercheurs dans l'adoption de JAX, atteignant 25~\% d'adoption en un an.
    }{
        % Spearheaded RLHF experiments to improve unsupervised skill discovery in embodied environments (iTHOR, Crafter).
        % Engineered a graphical skill extraction model in JAX/Flax, boosting training speed by 2–4x.
        Conducted reinforcement learning with human feedback (RLHF) experiments to enhance quality and alignment of unsupervised skill discovery in two embodied environments (iTHOR, Crafter).
        Fully implemented a graphical skill extraction model in JAX/Flax, achieving a 2--4x increase in training speed.
        Led lab-wide adoption of JAX, mentoring other researchers and driving \midTilde 25\% adoption within a year.  % ChkTeX 1
    }
}
\bulletItem{
    % ce dont je veux parler (pas forcément dans l'ordre):
    % developpement de l'environnement avec un framework pour creer des taches modulaire, et Conception de nouveaux algorithmes permettant de rendre calculables la fonction d'evaluation des agents, réduisant la surcharge à 1,7 % du temps d'exécution brut d'AI2THOR. deploimend de docker pour permettre l'entrainement des agents sur un cluster sans interface graphique
    \IfLanguageName{french}{
        Conception et développement d'un environnement d'apprentissage par renforcement incarné basé sur AI2THOR, incluant un framework pour la création de tâches composables.
        Conception de nouveaux algorithmes rendant calculable la fonction d'évaluation des agents, réduisant la surcharge à 1,7~\% du temps d'exécution brut d'AI2THOR\@.
        Déploiement de conteneurs Docker pour permettre l'entraînement des agents sur un cluster sans interface graphique.
    }{
        Led the design and development of an embodied reinforcement learning (RL) environment based on AI2THOR\@.
        Deployed Docker containers to run Unity simulations on a headless cluster, enabling efficient training of RL agents.
        Refined task evaluation algorithms, cutting overhead to 1.7\% of the raw simulation runtime.
        Designed 14 complex RL tasks, increasing task completion rates by 80\% compared to heuristic-based rewards.
        % Architected and deployed a custom embodied RL environment in AI2THOR, leveraging Docker for scalable Unity-based simulations on a headless cluster. Refined task evaluation algorithms, cutting overhead to 1.7% of simulation runtime. Designed 14 complex RL tasks, increasing task completion rates by 80% compared to heuristic-based rewards.
    }
}
\outerBulletListEnd{}


\companyNameAndLocationHeading{
    Apex Solutions
}{
    Figeac, France
}
\titleAndDateHeading{
    \IfLanguageName{french}{%
        \ifthenelse{\equal{\resumeFocus}{dev}}{Ingénieur Stagiaire}{Stage de recherche}
    }{%
        Research Internship
    }
}{
    \june{} 2022 \textbf{--} \august{} 2022
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        Développement d'un prototype d'environnement multi-agent pour la simulation de scénarios d'intrusion dans des infrastructures critiques (Python, Gym/Gymnasium), toujours activement utilisé pour la recherche interne.
    }{
        Prototyped a multi-agent simulation environment in Python (Gym/Gymnasium) to model intrusion scenarios in critical infrastructure.
        Developed a foundational framework still actively used for internal security research.
    }
}

\bulletItem{
    \IfLanguageName{french}{
        Conception et implémentation de benchmarks en apprentissage par renforcement (RL) pour des simulations multi-agents de capture-the-flag avec partage d'information limité (PyTorch, Stable-Baselines3).
        Adaptation et optimisation d'algorithmes de RL pour les agents d'attaque et de défense dans un contexte multi-agent.
    }{
        Developed RL benchmarks for multi-agent capture-the-flag simulations with realistic limited information-sharing constraints (PyTorch, Stable-Baselines3).
        Optimized RL algorithms for red team (penetration) and blue team (defense) agents in multi-agent scenarios, improving adversarial training in security simulations.
    }
}
\outerBulletListEnd{}


% \iftoggle{is_long_version}
% {
%
% }{}
