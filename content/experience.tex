% Experience section
\companyNameAndLocationHeading{
    SNU Vision \& Learning Lab
}{
    Seoul, South Korea
}
\titleAndDateHeading{
    Researcher
}{
    September 2022 \textbf{--} September 2024
}
\outerBulletListStart{}
\bulletItem{
    % Experimented reinforcement learning with human feedbacks (RLHF) to improve quality and alignment of unsupervised skill discovery in 2 embodied environments (iTHOR, Crafter).
    % Fully implemented graphical based skill extraction (HRL) method in JAX/Flax and improved training speed by 2 to 4 times.
    % Introduced and formed other researchers to JAX framework with \~25\% adoption in the lab within the following year.
    Conducted reinforcement learning with human feedback experiments to enhance quality and alignment of unsupervised skill discovery in two embodied environments (iTHOR, Crafter).
    Fully implemented a graphical skill extraction model in JAX/Flax, achieving a 2--4x increase in training speed.
    % Facilitated lab-wide adoption of the JAX framework, leading training sessions and achieving \~25\% adoption rate within a year.
    Facilitated lab-wide adoption of the JAX framework by introducing and supporting other researchers in its application, achieving \~25\% adoption within a year.
}
\bulletItem{
    % Lead modeling and development of an embodied RL environment based on AI2THOR focused on scalable  multi-task training with predicate based task evaluation.
    % Deployed containers (Docker) to run Unity based simulator with X11 server on headless cluster and train RL agents.
    % Optimized task evaluation algorithm and reduced overhead to 1.7\% of the raw AI2THOR simulation.
    % Designed 14 complex RL tasks and showed 80\% relative improvement when using the predicate based evaluation.
    % Paper currently under review for ICRA 2025.
    Led the design and development of an embodied RL environment based on AI2THOR
    Deployed Docker containers to run Unity-based simulations on a headless cluster, enabling efficient training of RL agents.
    Optimized task evaluation algorithms, reducing overhead to 1.7\% of the raw AI2THOR simulation runtime.
    Designed 14 complex RL tasks, demonstrating 80\% relative improvement in task completion with a predicate-based reward signal.
    Paper currently under review for ICRA 2025.
}
\outerBulletListEnd{}

\companyNameAndLocationHeading{
    Apex Solutions
}{
    Figeac, France
}
\titleAndDateHeading{
    Research internship
}{
    June 2022 \textbf{--} August 2022
}
\outerBulletListStart{}
\bulletItem{
    % Started modeling and development of a mulit-agent environment for the simulation of intrusion in critical infrastructures (Python, Gym/Gymnasium).
    % Base environment still in use within following research projects.
    Initiated the development of a multi-agent environment to simulate intrusion scenarios in critical infrastructure (Python, Gym/Gymnasium).
    Delivered a base environment still actively used in subsequent research projects.
}
\bulletItem{
    % Designed and implemented Reinforcement Learning (RL) benchmarks for multi-agent red team penetration and blue team defense (capture the flag) with limited information sharing.
    % Evaluation and experimentation of RL algorithms with adaptations for multi-agent settings (PyTorch, Stable-Baselines3).
    Designed and implemented reinforcement learning (RL) benchmarks for multi-agent capture-the-flag simulations with limited information-sharing constraints (PyTorch, Stable-Baselines3).
    Adapted RL algorithms for red team (penetration) and blue team (defense) agents to optimize performance in multi-agent scenarios.
    % Developed red team (penetration) and blue team (defense) agents, evaluating RL algorithms in multi-agent scenarios using PyTorch and Stable-Baselines3.
}
\outerBulletListEnd{}


% \iftoggle{is_long_version}
% {
%     \companyNameAndLocationHeading{}
%     {Autonomous Systems Laboratory}{Ithaca, NY}
%     \titleAndDateHeading{}
%     {Mechanical Engineering Researcher}{August 2014 \textbf{--} December 2014}
%     \outerBulletListStart{}
%     \bulletItem{Developed and wrote Python handlers to control a robotic ball, Sphero, with a path planning program, LTLMoP.}
%     \bulletItem{Successfully demoed the code, having Sphero autonomously traverse a map and react to its environment.}

%     \innerBulletListStart{}
%     \bulletItem{Co-implemented and \textbf{patented a novel Contingency MPC} for handling multi-modal predictions in trajectory optimizer, refactoring solver formulation to support joint optimization with minimal latency overhead.}
%     \bulletItem{Invented novel algorithm for generating \textbf{assertive lane change} trajectories via \enquote{gap matching} behavior to signal intent. Redesigned Planning/Control interface (C++) and added visualizations to support new feature.}
%     \bulletItem{\textbf{Tuned MPC} weights and enhanced cost function, contributing to investor analysts’ feedback that the ride was \textbf{\enquote{smoother than Waymo’s}}. First tuned in open-loop sim to match human driving, later fine-tuning on vehicle.}
%     \innerBulletListEnd{}

%     \outerBulletListEnd{}
% }{}
