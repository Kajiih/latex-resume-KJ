% % Experience section
\companyNameAndLocationHeading{
    SNU Vision \& Learning Lab
}{
    \IfLanguageName{french}{Séoul, Corée du Sud}{Seoul, South Korea}
}
\titleAndDateHeading{
    \IfLanguageName{french}{Étudiant-chercheur}{Graduate Student Researcher}
}{
    \september{} 2022 \textbf{--} \september{} 2024
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        Amélioration de la qualité et de l'alignement de la découverte non supervisée de compétences via l'apprentissage par renforcement avec rétroaction humaine (RLHF) pour des intelligences artificielles (IA) incarnées.
        Implémentation et optimisation d'un modèle probabiliste d'extraction de compétences en JAX/Flax, réduisant les coûts d'entraînement de 60\%.
        Formation et accompagnement des chercheurs dans l'adoption de JAX, atteignant 25\% d'adoption en un an.
    }{
        % Spearheaded RLHF experiments to improve unsupervised skill discovery in embodied environments (iTHOR, Crafter).
        % Engineered a graphical skill extraction model in JAX/Flax, boosting training speed by 2–4x.

        Conducted reinforcement learning with human feedback (RLHF) experiments to enhance quality and alignment of unsupervised skill discovery in two embodied environments (iTHOR, Crafter).
        Fully implemented a graphical skill extraction model in JAX/Flax, achieving a 2--4x increase in training speed.
        Led lab-wide adoption of JAX, mentoring other researchers and driving \midTilde 25\% adoption within a year.  % ChkTeX 1
    }

}
\bulletItem{
    \IfLanguageName{french}{
        Conception et développement d'un environnement d'apprentissage par renforcement incarné basé sur AI2THOR\@.
        Déploiement de conteneurs Docker pour exécuter des simulations Unity sur un cluster sans interface graphique, accélerant l'entraînement des agents.
        Optimisation des algorithmes d'évaluation, réduisant la surcharge à 1,7\% du temps d'exécution brut d'AI2THOR\@.
        % Conception de 14 tâches RL complexes, montrant une amélioration relative de 80\% du taux de succès grâce à un signal de récompense basé sur des prédicats.
    }{
        Led the design and development of an embodied reinforcement learning (RL) environment based on AI2THOR\@.
        Deployed Docker containers to run Unity simulations on a headless cluster, enabling efficient training of RL agents.
        Refined task evaluation algorithms, cutting overhead to 1.7\% of the raw simulation runtime.
        Designed 14 complex RL tasks, increasing task completion rates by 80\% compared to heuristic-based rewards.

        % Architected and deployed a custom embodied RL environment in AI2THOR, leveraging Docker for scalable Unity-based simulations on a headless cluster. Refined task evaluation algorithms, cutting overhead to 1.7% of simulation runtime. Designed 14 complex RL tasks, increasing task completion rates by 80% compared to heuristic-based rewards.
    }
}
\outerBulletListEnd{}


\companyNameAndLocationHeading{
    Apex Solutions
}{
    Figeac, France
}
\titleAndDateHeading{
    \IfLanguageName{french}{Stage de recherche}{Research Internship}
}{
    \june{} 2022 \textbf{--} \august{} 2022
}
\outerBulletListStart{}
\bulletItem{
    \IfLanguageName{french}{
        Développement d'un prototype d'environnement multi-agent pour la simulation de scénarios d'intrusion dans des infrastructures critiques (Python, Gym/Gymnasium), toujours activement utilisé pour la recherche interne.
    }{
        Prototyped a multi-agent simulation environment in Python (Gym/Gymnasium) to model intrusion scenarios in critical infrastructure.
        Developed a foundational framework still actively used for internal security research.
    }
}

\bulletItem{
    \IfLanguageName{french}{
        Conception et implémentation de benchmarks en apprentissage par renforcement (RL) pour des simulations multi-agents de capture-the-flag avec partage d'information limité (PyTorch, Stable-Baselines3).
        Adaptation et optimisation d'algorithmes de RL pour les agents d'attaque et de défense dans un contexte multi-agent.
    }{
        Developed RL benchmarks for multi-agent capture-the-flag simulations with realistic limited information-sharing constraints (PyTorch, Stable-Baselines3).
        Optimized RL algorithms for red team (penetration) and blue team (defense) agents in multi-agent scenarios, improving adversarial training in security simulations.
    }
}
\outerBulletListEnd{}


% \iftoggle{is_long_version}
% {
%
% }{}
